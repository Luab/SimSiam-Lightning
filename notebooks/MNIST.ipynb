{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malkalait\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "# regular imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Lightning import\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import Callback, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "# from pl_bolts.datamodules.mnist_datamodule import LightningDataModule, MNISTDataModule,\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "print(f'Cuda available: {torch.cuda.is_available()}')\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "# internal imports\n",
    "from src.callbacks import ImagePredictionLogger\n",
    "from src.dataset import ComposeMany, ComposeManyTorch, MNISTDataModule2\n",
    "from src.losses import log_softmax, simsiam_loss\n",
    "from src.models import accuracy, feature_std, CNN, BaseLitModel, SimSiam\n",
    "from src.utils import sweep_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentator\n",
    "\n",
    "AUG_KWARGS = dict(border_mode=A.cv2.BORDER_CONSTANT, value=0,\n",
    "                  interpolation=A.cv2.INTER_LANCZOS4)\n",
    "\n",
    "train_transforms = ComposeMany([\n",
    "    #A.RandomCrop(width=24, height=24),\n",
    "    #A.HorizontalFlip(p=0.5),\n",
    "    #A.GridDistortion(p=0.5, distort_limit=.3, **AUG_KWARGS),\n",
    "    A.ElasticTransform(p=0.5, sigma=1, alpha=3, alpha_affine=0, **AUG_KWARGS),\n",
    "    A.ElasticTransform(p=0.5, sigma=1, alpha=1, alpha_affine=3, **AUG_KWARGS),\n",
    "    A.ShiftScaleRotate(p=1.0, scale_limit=.2, rotate_limit=0, **AUG_KWARGS),\n",
    "    A.ShiftScaleRotate(p=1.0, scale_limit=0, rotate_limit=25, **AUG_KWARGS),\n",
    "    #A.CoarseDropout(p=1.0, max_holes=8, max_height=4, max_width=4,\n",
    "    #                min_holes=1, min_height=4, min_width=4),\n",
    "    #A.RandomBrightnessContrast(p=0.2),\n",
    "    #A.Blur(blur_limit=4),\n",
    "    A.Normalize(mean=(0.0,), std=(1,)),  # , max_pixel_value=255),\n",
    "    ToTensorV2()\n",
    "], n_aug=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lightning datamodule. Comes with its own train / val / test dataloader.\n",
    "mnist = MNISTDataModule2(data_dir='../data/', batch_size=512, train_transforms=train_transforms)\n",
    "mnist.prepare_data()\n",
    "mnist.setup()\n",
    "\n",
    "## Backbone arch\n",
    "cnn = CNN(num_channels=mnist.dims[0], num_classes=mnist.num_classes)\n",
    "\n",
    "## Metrics: (name of log-entry, metric)\n",
    "metrics = (('acc', accuracy),)\n",
    "\n",
    "model = BaseLitModel(\n",
    "    datamodule=mnist, backbone=cnn, loss_func=log_softmax, metrics=metrics,\n",
    "    lr=1e-3, flood_height=0.03\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(project='SimSiam-Lightning', job_type='train')\n",
    "callbacks = [\n",
    "    LearningRateMonitor(),  # log the LR\n",
    "    ImagePredictionLogger(mnist.val_dataloader(batch_size=32), n_samples=32),\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=200, gpus=-1,  # all GPUs\n",
    "    logger=wandb_logger, callbacks=callbacks,\n",
    "    accumulate_grad_batches=1, gradient_clip_val=0,  # 0.5\n",
    "    progress_bar_refresh_rate=20,\n",
    "    fast_dev_run=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# LR finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Learning rate finder\n",
    "# lr_finder = trainer.tuner.lr_find(model, num_training=3000, mode='linear', max_lr=1e-2)\n",
    "# # lr_finder.results  # Results can be found in\n",
    "# fig = lr_finder.plot(suggest=True)\n",
    "# lr_finder.suggestion()\n",
    "# model.hparams.lr = new_lr  # update hparams of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter sweep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.sweeps import sweep_config\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep_config, project=proj)\n",
    "\n",
    "# wandb.agent(sweep_id, function=sweep_iteration, project=proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SimSiam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentator\n",
    "AUG_KWARGS = dict(border_mode=A.cv2.BORDER_CONSTANT, value=0,\n",
    "                  interpolation=A.cv2.INTER_LANCZOS4)\n",
    "# transforms = ComposeMany([\n",
    "#     #A.ElasticTransform(p=0.5, sigma=1, alpha=3, alpha_affine=0, **AUG_KWARGS),\n",
    "#     #A.ElasticTransform(p=0.5, sigma=1, alpha=1, alpha_affine=3, **AUG_KWARGS),\n",
    "#     A.ShiftScaleRotate(p=1.0, scale_limit=.2, rotate_limit=0, **AUG_KWARGS),\n",
    "#     A.ShiftScaleRotate(p=1.0, scale_limit=0, rotate_limit=10, **AUG_KWARGS),\n",
    "#     A.Normalize(mean=(0.0,), std=(1,)),  # , max_pixel_value=255),\n",
    "#     ToTensorV2()\n",
    "# ], n_aug=2)\n",
    "\n",
    "transforms = ComposeManyTorch([\n",
    "    T.RandomResizedCrop(28, scale=(0.6, 1.0)),\n",
    "    #T.RandomHorizontalFlip(),\n",
    "    T.RandomApply([T.ColorJitter(0.4,0.4,0.4,0.1)], p=0.8),\n",
    "    #T.RandomGrayscale(p=0.2),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=28//20*2+1, sigma=(0.1, 2.0))], p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.1307], [0.3081]),\n",
    "], n_aug=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging metrics: ['featstd']\n"
     ]
    }
   ],
   "source": [
    "## Lightning datamodule. Comes with its own train / val / test dataloader.\n",
    "mnist = MNISTDataModule2(data_dir='../data/',\n",
    "                         batch_size=512,\n",
    "                         train_transforms=transforms,\n",
    "                         val_transforms=transforms)\n",
    "# mnist.prepare_data(); mnist.setup()\n",
    "\n",
    "## Backbone arch\n",
    "cnn = CNN(num_channels=mnist.dims[0], num_classes=mnist.num_classes)\n",
    "simsiam = SimSiam(backbone=cnn)\n",
    "\n",
    "model = BaseLitModel(\n",
    "    datamodule=mnist, backbone=simsiam,\n",
    "    loss_func=simsiam_loss,\n",
    "    metrics=(('featstd', feature_std),),\n",
    "    lr=0.05 * mnist.train_dataloader().batch_size / 256,  # 1e-3\n",
    "    #flood_height=0.03\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "Running in fast_dev_run mode: will run a full train, val and test loop using 1 batch(es).\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project='SimSiam-Lightning', job_type='train')\n",
    "callbacks = [\n",
    "    LearningRateMonitor(),  # log the LR\n",
    "    #ImagePredictionLogger(mnist.val_dataloader(batch_size=32), n_samples=32),\n",
    "]\n",
    "\n",
    "trainer = Trainer(\n",
    "    weights_summary='full',\n",
    "    max_epochs=200, #gpus=-1,  # all GPUs\n",
    "    logger=wandb_logger, callbacks=callbacks,\n",
    "    accumulate_grad_batches=1, gradient_clip_val=0,  # 0.5\n",
    "    progress_bar_refresh_rate=20,\n",
    "    fast_dev_run=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/freddie/venv/wotus/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: attribute 'backbone' removed from hparams because it cannot be pickled\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/freddie/venv/wotus/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: attribute 'metrics' removed from hparams because it cannot be pickled\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "   | Name                                       | Type              | Params | In sizes       | Out sizes                           \n",
      "------------------------------------------------------------------------------------------------------------------------------------------\n",
      "0  | backbone                                   | SimSiam           | 20.1 K | [1, 2, 28, 28] | [[1, 32], [1, 32], [1, 32], [1, 32]]\n",
      "1  | backbone.f                                 | Sequential        | 19.0 K | [1, 1, 28, 28] | [1, 32]                             \n",
      "2  | backbone.f.features                        | Sequential        | 17.9 K | [1, 1, 28, 28] | [1, 32, 1, 1]                       \n",
      "3  | backbone.f.features.conv1                  | Sequential        | 48     | [1, 1, 28, 28] | [1, 4, 14, 14]                      \n",
      "4  | backbone.f.features.conv1.conv2d           | Conv2d            | 40     | [1, 1, 28, 28] | [1, 4, 28, 28]                      \n",
      "5  | backbone.f.features.conv1.bn               | BatchNorm2d       | 8      | [1, 4, 28, 28] | [1, 4, 28, 28]                      \n",
      "6  | backbone.f.features.conv1.relu             | LeakyReLU         | 0      | [1, 4, 28, 28] | [1, 4, 28, 28]                      \n",
      "7  | backbone.f.features.conv1.maxpool          | MaxPool2d         | 0      | [1, 4, 28, 28] | [1, 4, 14, 14]                      \n",
      "8  | backbone.f.features.conv2                  | Sequential        | 312    | [1, 4, 14, 14] | [1, 8, 7, 7]                        \n",
      "9  | backbone.f.features.conv2.conv2d           | Conv2d            | 296    | [1, 4, 14, 14] | [1, 8, 14, 14]                      \n",
      "10 | backbone.f.features.conv2.bn               | BatchNorm2d       | 16     | [1, 8, 14, 14] | [1, 8, 14, 14]                      \n",
      "11 | backbone.f.features.conv2.relu             | LeakyReLU         | 0      | [1, 8, 14, 14] | [1, 8, 14, 14]                      \n",
      "12 | backbone.f.features.conv2.maxpool          | MaxPool2d         | 0      | [1, 8, 14, 14] | [1, 8, 7, 7]                        \n",
      "13 | backbone.f.features.twoconv1               | Sequential        | 3.6 K  | [1, 8, 7, 7]   | [1, 16, 3, 3]                       \n",
      "14 | backbone.f.features.twoconv1.conv1         | Sequential        | 1.2 K  | [1, 8, 7, 7]   | [1, 16, 7, 7]                       \n",
      "15 | backbone.f.features.twoconv1.conv1.conv2d  | Conv2d            | 1.2 K  | [1, 8, 7, 7]   | [1, 16, 7, 7]                       \n",
      "16 | backbone.f.features.twoconv1.conv1.bn      | BatchNorm2d       | 32     | [1, 16, 7, 7]  | [1, 16, 7, 7]                       \n",
      "17 | backbone.f.features.twoconv1.conv1.relu    | LeakyReLU         | 0      | [1, 16, 7, 7]  | [1, 16, 7, 7]                       \n",
      "18 | backbone.f.features.twoconv1.conv2         | Sequential        | 2.4 K  | [1, 16, 7, 7]  | [1, 16, 3, 3]                       \n",
      "19 | backbone.f.features.twoconv1.conv2.conv2d  | Conv2d            | 2.3 K  | [1, 16, 7, 7]  | [1, 16, 7, 7]                       \n",
      "20 | backbone.f.features.twoconv1.conv2.bn      | BatchNorm2d       | 32     | [1, 16, 7, 7]  | [1, 16, 7, 7]                       \n",
      "21 | backbone.f.features.twoconv1.conv2.relu    | LeakyReLU         | 0      | [1, 16, 7, 7]  | [1, 16, 7, 7]                       \n",
      "22 | backbone.f.features.twoconv1.conv2.maxpool | MaxPool2d         | 0      | [1, 16, 7, 7]  | [1, 16, 3, 3]                       \n",
      "23 | backbone.f.features.twoconv2               | Sequential        | 14.0 K | [1, 16, 3, 3]  | [1, 32, 1, 1]                       \n",
      "24 | backbone.f.features.twoconv2.conv1         | Sequential        | 4.7 K  | [1, 16, 3, 3]  | [1, 32, 3, 3]                       \n",
      "25 | backbone.f.features.twoconv2.conv1.conv2d  | Conv2d            | 4.6 K  | [1, 16, 3, 3]  | [1, 32, 3, 3]                       \n",
      "26 | backbone.f.features.twoconv2.conv1.bn      | BatchNorm2d       | 64     | [1, 32, 3, 3]  | [1, 32, 3, 3]                       \n",
      "27 | backbone.f.features.twoconv2.conv1.relu    | LeakyReLU         | 0      | [1, 32, 3, 3]  | [1, 32, 3, 3]                       \n",
      "28 | backbone.f.features.twoconv2.conv2         | Sequential        | 9.3 K  | [1, 32, 3, 3]  | [1, 32, 1, 1]                       \n",
      "29 | backbone.f.features.twoconv2.conv2.conv2d  | Conv2d            | 9.2 K  | [1, 32, 3, 3]  | [1, 32, 3, 3]                       \n",
      "30 | backbone.f.features.twoconv2.conv2.bn      | BatchNorm2d       | 64     | [1, 32, 3, 3]  | [1, 32, 3, 3]                       \n",
      "31 | backbone.f.features.twoconv2.conv2.relu    | LeakyReLU         | 0      | [1, 32, 3, 3]  | [1, 32, 3, 3]                       \n",
      "32 | backbone.f.features.twoconv2.conv2.maxpool | MaxPool2d         | 0      | [1, 32, 3, 3]  | [1, 32, 1, 1]                       \n",
      "33 | backbone.f.avgpoool                        | AdaptiveAvgPool2d | 0      | [1, 32, 1, 1]  | [1, 32, 1, 1]                       \n",
      "34 | backbone.f.flatten                         | Flatten           | 0      | [1, 32, 1, 1]  | [1, 32]                             \n",
      "35 | backbone.f.fc                              | Sequential        | 1.1 K  | [1, 32]        | [1, 32]                             \n",
      "36 | backbone.f.fc.linear                       | Linear            | 1.1 K  | [1, 32]        | [1, 32]                             \n",
      "37 | backbone.f.fc.relu                         | LeakyReLU         | 0      | [1, 32]        | [1, 32]                             \n",
      "38 | backbone.f.fc.dropout                      | Dropout           | 0      | [1, 32]        | [1, 32]                             \n",
      "39 | backbone.h                                 | Sequential        | 1.1 K  | [1, 32]        | [1, 32]                             \n",
      "40 | backbone.h.fc1                             | Sequential        | 528    | [1, 32]        | [1, 16]                             \n",
      "41 | backbone.h.fc1.linear                      | Linear            | 528    | [1, 32]        | [1, 16]                             \n",
      "42 | backbone.h.fc1.relu                        | LeakyReLU         | 0      | [1, 16]        | [1, 16]                             \n",
      "43 | backbone.h.fc1.dropout                     | Dropout           | 0      | [1, 16]        | [1, 16]                             \n",
      "44 | backbone.h.fc2                             | Sequential        | 544    | [1, 16]        | [1, 32]                             \n",
      "45 | backbone.h.fc2.linear                      | Linear            | 544    | [1, 16]        | [1, 32]                             \n",
      "46 | backbone.h.fc2.relu                        | LeakyReLU         | 0      | [1, 32]        | [1, 32]                             \n",
      "47 | backbone.h.fc2.dropout                     | Dropout           | 0      | [1, 32]        | [1, 32]                             \n",
      "------------------------------------------------------------------------------------------------------------------------------------------\n",
      "20.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.1 K    Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44fb9b8337445ffb1e8bdfdce96d3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.losses import negcosim, simsiam_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simsiam.train(False);\n",
    "print(simsiam.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.rand((1,) + mnist.dims).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y = next(iter(mnist.train_dataloader()))\n",
    "x_val, y = next(iter(mnist.val_dataloader()))\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn.avgpool(cnn.features(x[:,[0]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = x_train[:, [0]], x_train[:, [1]]\n",
    "# x1, x2 = x_val[:, [0]], x_val[:, [1]]\n",
    "print(x1.shape)\n",
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((x1.view(512,-1) - x2.view(512,-1))**2).mean())\n",
    "print(negcosim(x1.view(512,-1), x2.view(512,-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 112\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(x1[i][0], );\n",
    "ax[1].imshow(x2[i][0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(((x1.view(512,-1) - x2.view(512,-1))**2).mean(dim=0).numpy(), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows that dropout is enough to cause a difference between x1 and x2 during training. When dropout is paused in validation, f yields identical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z1, z2, p1, p2 = simsiam(x_train)\n",
    "# z1, z2, p1, p2 = simsiam(x_val)\n",
    "z1, z2 = simsiam.f(x1), simsiam.f(x2)  # projections\n",
    "p1, p2 = simsiam.h(z1), simsiam.h(z2)  # centroid predictions\n",
    "\n",
    "print(((z1 - z2)**2).mean())\n",
    "print(negcosim(z1, z2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simsiam_loss((x_train, y), simsiam)\n",
    "# simsiam_loss((x_val, y), simsiam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((F.normalize(z1) - F.normalize(p1))**2).mean())\n",
    "print(((F.normalize(z2) - F.normalize(p2))**2).mean())\n",
    "print(((F.normalize(z1) - F.normalize(z2))**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(negcosim(p1, z2))\n",
    "print(negcosim(p2, z1))\n",
    "print((negcosim(p1, z2) + negcosim(p2, z1)) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from src.dataset import ComposeManyTorch\n",
    "\n",
    "transforms = ComposeManyTorch([\n",
    "    T.RandomResizedCrop(28, scale=(0.6, 1.0)),\n",
    "    #T.RandomHorizontalFlip(),\n",
    "    T.RandomApply([T.ColorJitter(0.4,0.4,0.4,0.1)], p=0.8),\n",
    "    #T.RandomGrayscale(p=0.2),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=28//20*2+1, sigma=(0.1, 2.0))], p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.1307], [0.3081]),\n",
    "], n_aug=2)\n",
    "\n",
    "mnist = MNISTDataModule2(data_dir='../data/',\n",
    "                         batch_size=32,\n",
    "                         train_transforms=transforms,\n",
    "                         val_transforms=transforms)\n",
    "\n",
    "x, y = next(iter(mnist.val_dataloader()))\n",
    "\n",
    "i = 1\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(x[i][0], );\n",
    "ax[1].imshow(x[i][1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,0].std(dim=0).mean()\n",
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
